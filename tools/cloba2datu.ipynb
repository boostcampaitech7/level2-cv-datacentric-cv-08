{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet 이미지, json 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_img_json(parquet_path, img_dir, json_dir, sample_size=200,image_only=False):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    image_data_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        img_data = row['image']['bytes']\n",
    "        img = Image.open(BytesIO(img_data))\n",
    "        \n",
    "        if not image_only:\n",
    "            gt_str = row['ground_truth']\n",
    "            gt_dict = json.loads(gt_str)\n",
    "            image_id = gt_dict['meta']['image_id']\n",
    "        else:\n",
    "            image_id = f\"{idx}\"\n",
    "            \n",
    "        with BytesIO() as output:\n",
    "            img.save(output, format=\"jpeg\")\n",
    "            image_size = output.tell()\n",
    "            \n",
    "        if image_only:\n",
    "            image_data_list.append((image_id, img_data, image_size))\n",
    "        else:\n",
    "            image_data_list.append((image_id, img_data, gt_dict, image_size))\n",
    "        \n",
    "    image_data_list.sort(key=lambda x: x[2 if image_only else 3], reverse=True)\n",
    "    selected_images = image_data_list[:sample_size]\n",
    "    \n",
    "    for data in selected_images:\n",
    "        image_id, img_data = data[0], data[1]\n",
    "        img = Image.open(BytesIO(img_data))\n",
    "\n",
    "        img_path = os.path.join(img_dir, f\"image_{image_id}.jpg\")\n",
    "        img.save(img_path)\n",
    "\n",
    "        if not image_only and len(data) > 2:\n",
    "            gt_dict = data[2]\n",
    "            if gt_dict is not None:\n",
    "                gt_path = os.path.join(json_dir, f\"{image_id}.json\")\n",
    "                with open(gt_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(gt_dict, f, ensure_ascii=False, indent=4)\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parquet_path (str) : parquet 경로\n",
    "image_dir (str) : parquet 에서 추출한 이미지 저장 경로\n",
    "image_dir (str) : parquet 에서 추출한 json 저장 경로\n",
    "'''\n",
    "\n",
    "parquet_path = \"/data/ephemeral/home/cordx/parquet/zh.parquet\"\n",
    "image_dir = \"/data/ephemeral/home/cordx/cord_zh_image\"\n",
    "json_dir = \"/data/ephemeral/home/cordx/cord_zh_json\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(json_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "make_img_json(parquet_path=parquet_path,\n",
    "              img_dir=image_dir,\n",
    "              json_dir=json_dir,\n",
    "              image_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORD to Datumaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cloba2datu(json_folder, output_path):\n",
    "    dataset = {\n",
    "        \"info\": {},\n",
    "        \"categories\": {\n",
    "            \"label\": {\n",
    "                \"labels\": [\n",
    "                    {\n",
    "                        \"name\": \"text\",\n",
    "                        \"parent\": \"\",\n",
    "                        \"attributes\": []\n",
    "                    }\n",
    "                ],\n",
    "                \"attributes\": []\n",
    "            }\n",
    "        },\n",
    "        \"items\": []\n",
    "    }\n",
    "\n",
    "    label_name_to_id = {\"text\": 0}\n",
    "    annotation_id = 0\n",
    "    json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "    json_files.sort()\n",
    "\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        image_id =\"image_\" + data['meta']['image_id']\n",
    "        image_size = data['meta']['image_size']\n",
    "        width = image_size['width']\n",
    "        height = image_size['height']\n",
    "        image_path = f\"{image_id}.jpg\"\n",
    "\n",
    "        annotations = []\n",
    "\n",
    "        for line in data['valid_line']:\n",
    "            for word_info in line['words']:\n",
    "                quad = word_info['quad']\n",
    "                points = [\n",
    "                    quad['x1'], quad['y1'],\n",
    "                    quad['x2'], quad['y2'],\n",
    "                    quad['x3'], quad['y3'],\n",
    "                    quad['x4'], quad['y4']\n",
    "                ]\n",
    "\n",
    "                text = word_info.get('text', '')\n",
    "                is_key = word_info.get('is_key', 0)\n",
    "                row_id = word_info.get('row_id', None)\n",
    "\n",
    "                annotation = {\n",
    "                    \"id\": annotation_id,\n",
    "                    \"type\": \"polygon\",\n",
    "                    \"attributes\": {\n",
    "                        \"text\": text,\n",
    "                        \"is_key\": is_key,\n",
    "                        \"row_id\": row_id\n",
    "                    },\n",
    "                    \"group\": 0,\n",
    "                    \"label_id\": label_name_to_id[\"text\"],\n",
    "                    \"points\": [float(coord) for coord in points],\n",
    "                    \"z_order\": 0\n",
    "                }\n",
    "                annotations.append(annotation)\n",
    "                annotation_id += 1\n",
    "\n",
    "        item = {\n",
    "            \"id\": str(image_id),\n",
    "            \"annotations\": annotations,\n",
    "            \"image\": {\n",
    "                \"size\": [int(height), int(width)],\n",
    "                \"path\": image_path\n",
    "            }\n",
    "        }\n",
    "        dataset['items'].append(item)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Datumaro 포맷의 JSON 파일이 생성되었습니다: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datumaro 포맷의 JSON 파일이 생성되었습니다: cloba2datu.json\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "output_path (str) : Datumaro 포맷으로 변환한 결과 저장 경로\n",
    "'''\n",
    "\n",
    "output_path = \"cloba2datu.json\"\n",
    "\n",
    "make_cloba2datu(json_dir, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
